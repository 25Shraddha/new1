{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "SpaCy\n",
      "is\n",
      "a\n",
      "popular\n",
      "library\n",
      "for\n",
      "NLP\n",
      "tasks\n",
      ".\n",
      "It\n",
      "is\n",
      "fast\n",
      "and\n",
      "efficient\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Tokenization....................\n",
    "#  Step 1: Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Step 2: Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 3: Define the text to be tokenized\n",
    "text = \"SpaCy is a popular library for NLP tasks. It is fast and efficient.\"\n",
    "\n",
    "# Step 4: Process the text using the NLP model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Step 5: Tokenize and print tokens\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      "Original: The, Lemma: the\n",
      "Original: children, Lemma: child\n",
      "Original: are, Lemma: be\n",
      "Original: playing, Lemma: play\n",
      "Original: with, Lemma: with\n",
      "Original: their, Lemma: their\n",
      "Original: toys, Lemma: toy\n",
      "Original: ., Lemma: .\n",
      "Original: They, Lemma: they\n",
      "Original: played, Lemma: play\n",
      "Original: all, Lemma: all\n",
      "Original: day, Lemma: day\n",
      "Original: yesterday, Lemma: yesterday\n",
      "Original: ., Lemma: .\n"
     ]
    }
   ],
   "source": [
    "#lemmatization..................\n",
    "# Step 1: Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Step 2: Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 3: Define the text\n",
    "text = \"The children are playing with their toys. They played all day yesterday.\"\n",
    "\n",
    "# Step 4: Process the text with the NLP model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Step 5: Perform lemmatization\n",
    "print(\"Lemmatized Tokens:\")\n",
    "for token in doc:\n",
    "    print(f\"Original: {token.text}, Lemma: {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a simple example demonstrating stop word removal using spaCy.\n",
      "Text after Stop Word Removal: simple example demonstrating stop word removal spaCy .\n"
     ]
    }
   ],
   "source": [
    "#Stop word Removal\n",
    "# Step 1: Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Step 2: Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 3: Define the text\n",
    "text = \"This is a simple example demonstrating stop word removal using spaCy.\"\n",
    "\n",
    "# Step 4: Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Step 5: Filter out stop words\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Step 6: Display the result\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Text after Stop Word Removal:\", \" \".join(filtered_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary:\n",
      "{'fascinating': 0, 'is': 1, 'language': 2, 'natural': 3, 'processing': 4, 'about': 5, 'i': 6, 'learning': 7, 'love': 8, 'nlp': 9, 'a': 10, 'bag': 11, 'concept': 12, 'fundamental': 13, 'in': 14, 'model': 15, 'of': 16, 'the': 17, 'words': 18, 'and': 19, 'easier': 20, 'efficient': 21, 'gensim': 22, 'makes': 23, 'more': 24}\n",
      "\n",
      "Bag-of-Words Corpus:\n",
      "Document 1: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "Document 2: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n",
      "Document 3: [(1, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]\n",
      "Document 4: [(9, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]\n",
      "\n",
      "TF-IDF Corpus:\n",
      "Document 1: [(0, 0.48507125007266594), (1, 0.24253562503633297), (2, 0.48507125007266594), (3, 0.48507125007266594), (4, 0.48507125007266594)]\n",
      "Document 2: [(5, 0.49733003742454074), (6, 0.49733003742454074), (7, 0.49733003742454074), (8, 0.49733003742454074), (9, 0.10320530752446756)]\n",
      "Document 3: [(1, 0.1426067007488231), (9, 0.05918712845919794), (10, 0.2852134014976462), (11, 0.5704268029952924), (12, 0.2852134014976462), (13, 0.2852134014976462), (14, 0.2852134014976462), (15, 0.2852134014976462), (16, 0.2852134014976462), (17, 0.2852134014976462), (18, 0.2852134014976462)]\n",
      "Document 4: [(9, 0.08441677254117616), (19, 0.4067910619539494), (20, 0.4067910619539494), (21, 0.4067910619539494), (22, 0.4067910619539494), (23, 0.4067910619539494), (24, 0.4067910619539494)]\n"
     ]
    }
   ],
   "source": [
    "#Assignment 2............Assignment to implement Bag of Words and TFIDF using Gensim library.\n",
    "# Importing required libraries\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "\n",
    "# Sample corpus (list of documents)\n",
    "corpus = [\n",
    "    \"Natural Language Processing is fascinating\",\n",
    "    \"I love learning about NLP\",\n",
    "    \"The Bag of Bag Words model is a fundamental concept in NLP\",\n",
    "    \"Gensim makes NLP easier and more efficient\",\n",
    "]\n",
    "\n",
    "# Preprocessing: Tokenization\n",
    "# Convert each document into a list of words\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# Step 1: Create a dictionary\n",
    "dictionary = Dictionary(tokenized_corpus)\n",
    "\n",
    "# Step 2: Convert the documents into a bag-of-words format\n",
    "# (list of tuples (token_id, token_count) for each document)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "\n",
    "# Display the dictionary and the BoW representation\n",
    "print(\"Dictionary:\")\n",
    "print(dictionary.token2id)\n",
    "print(\"\\nBag-of-Words Corpus:\")\n",
    "for i, bow in enumerate(bow_corpus):\n",
    "    print(f\"Document {i+1}: {bow}\")\n",
    "\n",
    "# Step 3: Compute the term frequency-inverse document frequency (TF-IDF) scores (optional)\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "# Display the TF-IDF representation\n",
    "print(\"\\nTF-IDF Corpus:\")\n",
    "for i, tfidf_doc in enumerate(tfidf_corpus):\n",
    "    print(f\"Document {i+1}: {tfidf_doc}\")\n",
    "\n",
    "# Additional Information:\n",
    "# The `dictionary` maps words to their integer IDs.\n",
    "# The `bow_corpus` represents each document as a sparse vector of (word_id, frequency).\n",
    "# The `tfidf_corpus` shows TF-IDF weights for words in each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, Phrases, and Concepts:\n",
      "Albert Einstein (PERSON)\n",
      "Ulm (GPE)\n",
      "Germany (GPE)\n",
      "March 14, 1879 (DATE)\n",
      "the Nobel Prize in Physics (WORK_OF_ART)\n",
      "1921 (DATE)\n",
      "Google (ORG)\n",
      "September 1998 (DATE)\n",
      "Larry Page (PERSON)\n",
      "Sergey Brin (PERSON)\n",
      "Ph.D. (WORK_OF_ART)\n",
      "Stanford University (ORG)\n"
     ]
    }
   ],
   "source": [
    "#Assignment 3.................... Name Entity Recognition in python with spacy\n",
    "# Step 1: Install spaCy if you haven't already\n",
    "# pip install spacy\n",
    "\n",
    "# Step 2: Download the English language model\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Textual data to analyze\n",
    "text = \"\"\"\n",
    "Albert Einstein was a theoretical physicist who developed the theory of relativity. \n",
    "He was born in Ulm, Germany, on March 14, 1879. Einstein won the Nobel Prize in Physics in 1921. \n",
    "Google was founded in September 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities, their labels, and positions\n",
    "print(\"Named Entities, Phrases, and Concepts:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********   UNIGRAM    ************************\n",
      "('Earth',)\n",
      "('is',)\n",
      "('the',)\n",
      "('third',)\n",
      "('planet',)\n",
      "('from',)\n",
      "('the',)\n",
      "('Sun',)\n",
      "('in',)\n",
      "('our',)\n",
      "('solar',)\n",
      "('system',)\n",
      "('and',)\n",
      "('the',)\n",
      "('only',)\n",
      "('known',)\n",
      "('celestial',)\n",
      "('body',)\n",
      "('to',)\n",
      "('support',)\n",
      "('life.',)\n",
      "('With',)\n",
      "('a',)\n",
      "('diverse',)\n",
      "('range',)\n",
      "('of',)\n",
      "('ecosystems,',)\n",
      "('it',)\n",
      "('is',)\n",
      "('home',)\n",
      "('to',)\n",
      "('a',)\n",
      "('vast',)\n",
      "('array',)\n",
      "('of',)\n",
      "('plant',)\n",
      "('and',)\n",
      "('animal',)\n",
      "('species,',)\n",
      "('including',)\n",
      "('humans.',)\n",
      "\n",
      "***********   BIGRAM    ************************\n",
      "('Earth', 'is')\n",
      "('is', 'the')\n",
      "('the', 'third')\n",
      "('third', 'planet')\n",
      "('planet', 'from')\n",
      "('from', 'the')\n",
      "('the', 'Sun')\n",
      "('Sun', 'in')\n",
      "('in', 'our')\n",
      "('our', 'solar')\n",
      "('solar', 'system')\n",
      "('system', 'and')\n",
      "('and', 'the')\n",
      "('the', 'only')\n",
      "('only', 'known')\n",
      "('known', 'celestial')\n",
      "('celestial', 'body')\n",
      "('body', 'to')\n",
      "('to', 'support')\n",
      "('support', 'life.')\n",
      "('life.', 'With')\n",
      "('With', 'a')\n",
      "('a', 'diverse')\n",
      "('diverse', 'range')\n",
      "('range', 'of')\n",
      "('of', 'ecosystems,')\n",
      "('ecosystems,', 'it')\n",
      "('it', 'is')\n",
      "('is', 'home')\n",
      "('home', 'to')\n",
      "('to', 'a')\n",
      "('a', 'vast')\n",
      "('vast', 'array')\n",
      "('array', 'of')\n",
      "('of', 'plant')\n",
      "('plant', 'and')\n",
      "('and', 'animal')\n",
      "('animal', 'species,')\n",
      "('species,', 'including')\n",
      "('including', 'humans.')\n",
      "\n",
      "***********   TRIGRAM    ************************\n",
      "('Earth', 'is', 'the')\n",
      "('is', 'the', 'third')\n",
      "('the', 'third', 'planet')\n",
      "('third', 'planet', 'from')\n",
      "('planet', 'from', 'the')\n",
      "('from', 'the', 'Sun')\n",
      "('the', 'Sun', 'in')\n",
      "('Sun', 'in', 'our')\n",
      "('in', 'our', 'solar')\n",
      "('our', 'solar', 'system')\n",
      "('solar', 'system', 'and')\n",
      "('system', 'and', 'the')\n",
      "('and', 'the', 'only')\n",
      "('the', 'only', 'known')\n",
      "('only', 'known', 'celestial')\n",
      "('known', 'celestial', 'body')\n",
      "('celestial', 'body', 'to')\n",
      "('body', 'to', 'support')\n",
      "('to', 'support', 'life.')\n",
      "('support', 'life.', 'With')\n",
      "('life.', 'With', 'a')\n",
      "('With', 'a', 'diverse')\n",
      "('a', 'diverse', 'range')\n",
      "('diverse', 'range', 'of')\n",
      "('range', 'of', 'ecosystems,')\n",
      "('of', 'ecosystems,', 'it')\n",
      "('ecosystems,', 'it', 'is')\n",
      "('it', 'is', 'home')\n",
      "('is', 'home', 'to')\n",
      "('home', 'to', 'a')\n",
      "('to', 'a', 'vast')\n",
      "('a', 'vast', 'array')\n",
      "('vast', 'array', 'of')\n",
      "('array', 'of', 'plant')\n",
      "('of', 'plant', 'and')\n",
      "('plant', 'and', 'animal')\n",
      "('and', 'animal', 'species,')\n",
      "('animal', 'species,', 'including')\n",
      "('species,', 'including', 'humans.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import ngrams\n",
    "\n",
    "# Input text\n",
    "sentence = 'Earth is the third planet from the Sun in our solar system and the only known celestial body to support life. With a diverse range of ecosystems, it is home to a vast array of plant and animal species, including humans.'\n",
    "\n",
    "# Unigram model\n",
    "n = 1\n",
    "unigrams = ngrams(sentence.split(), n)\n",
    "print(f\"\\n***********   UNIGRAM    ************************\")\n",
    "for item in unigrams:\n",
    "    print(item)\n",
    "\n",
    "# Bigram model\n",
    "n = 2\n",
    "bigrams = ngrams(sentence.split(), n)\n",
    "print(f\"\\n***********   BIGRAM    ************************\")\n",
    "for item in bigrams:\n",
    "    print(item)\n",
    "\n",
    "# Trigram model\n",
    "n = 3\n",
    "trigrams = ngrams(sentence.split(), n)\n",
    "print(f\"\\n***********   TRIGRAM    ************************\")\n",
    "for item in trigrams:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs: ['https://www.example.com', 'http://testsite.com']\n",
      "IP Addresses: ['192.168.0.1', '8.8.8.8']\n",
      "Dates: ['12/09/2023', '2023-09-12', '31-12-2023']\n",
      "PAN Numbers: ['ABCDE1234F', 'XYWZP9876L']\n"
     ]
    }
   ],
   "source": [
    "#Assignment 5.............Implement regular expression function to find URL, IP address, Date,\n",
    "#PAN number in textual data using python libraries\n",
    "import re\n",
    "\n",
    "def find_urls(text):\n",
    "    url_pattern = r\"https?://(?:www\\.)?\\S+|www\\.\\S+\"\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "def find_ip_addresses(text):\n",
    "    ip_pattern = r\"\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b\"\n",
    "    return re.findall(ip_pattern, text)\n",
    "\n",
    "def find_dates(text):\n",
    "    date_pattern = r\"\\b(?:\\d{1,2}[-/]){2}\\d{2,4}\\b|\\b\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\\b\"\n",
    "    return re.findall(date_pattern, text)\n",
    "\n",
    "def find_pan_numbers(text):\n",
    "    pan_pattern = r\"\\b[A-Z]{5}[0-9]{4}[A-Z]\\b\"\n",
    "    return re.findall(pan_pattern, text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"Here are some examples:\n",
    "    - URL: https://www.example.com and http://testsite.com\n",
    "    - IP: 192.168.0.1 and 8.8.8.8\n",
    "    - Dates: 12/09/2023, 2023-09-12, and 31-12-2023\n",
    "    - PAN: ABCDE1234F and XYWZP9876L\n",
    "    \"\"\"\n",
    "\n",
    "    urls = find_urls(sample_text)\n",
    "    ip_addresses = find_ip_addresses(sample_text)\n",
    "    dates = find_dates(sample_text)\n",
    "    pan_numbers = find_pan_numbers(sample_text)\n",
    "\n",
    "    print(\"URLs:\", urls)\n",
    "    print(\"IP Addresses:\", ip_addresses)\n",
    "    print(\"Dates:\", dates)\n",
    "    print(\"PAN Numbers:\", pan_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: \n",
      "\n",
      "=====\n",
      "token.tag_ = '_SP'\n",
      "token.head.text = 'I'\n",
      "token.dep_ = 'dep'\n",
      "\n",
      "TOKEN: I\n",
      "=====\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'prefer'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: prefer\n",
      "=====\n",
      "token.tag_ = 'VBP'\n",
      "token.head.text = 'prefer'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'flight'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: morning\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'flight'\n",
      "token.dep_ = 'compound'\n",
      "\n",
      "TOKEN: flight\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'prefer'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: through\n",
      "=====\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'flight'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: denver\n",
      "=====\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'through'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: \n",
      "\n",
      "=====\n",
      "token.tag_ = '_SP'\n",
      "token.head.text = 'denver'\n",
      "token.dep_ = 'dep'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"45132808c255428bbdc704c7cc2fbc7c-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">\n",
       "</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SPACE</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">prefer</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">morning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">flight</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">through</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">denver</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">\n",
       "</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SPACE</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-45132808c255428bbdc704c7cc2fbc7c-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-45132808c255428bbdc704c7cc2fbc7c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "#Assignment 6..................\n",
    "###  Assignment No 6 ###\n",
    "#Name : Shital Rahane\n",
    "#Batch : B3\n",
    "#Roll No : 48\n",
    "\"\"\"Assignment Title : : Implement and visualize Dependency Parsing of Textual Input\n",
    "using Stan- ford CoreNLP and Spacy library\"\"\"\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "multiline_text = \"\"\"\n",
    "I prefer the morning flight through denver\n",
    "\"\"\"\n",
    "\n",
    "multiline_doc = nlp(multiline_text)\n",
    "\n",
    "for token in multiline_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {token.text}\n",
    "=====\n",
    "{token.tag_ = }\n",
    "{token.head.text = }\n",
    "{token.dep_ = }\"\"\"\n",
    "    )\n",
    "\n",
    "displacy.serve(multiline_doc, style=\"dep\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
